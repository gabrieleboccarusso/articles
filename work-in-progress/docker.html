<!-- title: deploy everything with ease - docker from the ground up -->

<ol>
 <li>installing docker
   <ol>
    <li>ubuntu (linux)</li>
    <li>Windows</li>
    <li>MacOs</li>
   </ol>
 </li>
</ol>

<p>
 Docker is a way to deploy software by storing it in lightweight packages.
 <br>
 to check if docker is correctly installed run on the cli the command <code>docker -v</code>
</p>

<h2>What is <i>DevOps</i>?</h2>
<p>
 Docker is getting everyday more relevant into the world of DevOps and to fully understand it is important to make clear what DevOps is and what role plays into the modern software development scene.
 <br>
 <i>Dev</i> refers to software development and <i>Ops</i> to the related operations regarding the deployment, the release and the monitoring of the performance. DevOps bridges the gap between the development and the release of a software using a set of precise practices aimed to improve the workflow between quality assurance, continous development and continous integration of the software throught its whole lifecycle.
</p>
<p>
 IMAGE1
</p>

<h2>What is Docker?</h2>
<p>
 Docker is a set of products that use virtualization at the Operative System level to delived software in packages called containers. A docker container contains mainly two things:
 <ul>
  <li>Software source code</li>
  <li>Operative System files (binaries and libraries)</li>
 </ul>
 These two things, although being in the same container, are throughly separated so to not interfere with each other or other softwares. If such need would eventually arise Docker offers way and tools to do it.
 <br>
 IMAGE2
</p>
<p>
 The containers are the actual instances of an application. The source code to create a together (as to put all the different pieces together) is container in a file called <code>Dockerfile</code> and that is referenced as an <b>image</b>. An image is the source code of a container and a container is an instance of an image.
</p>

<h2>Docker's benefits</h2>
<p>
 Containerizing an application provides multiple benefits. First of them is to make an application agnostic from the computer where it is develop and from the server. With the container already having all that is needed to run the application there is the certainty that it will run the same on the development machine, on the server and on every different platform it is used thus being indipendent from the outside environment. 
</p>
<p>
 If a software needs a dependency as, for example, a specific version of a database, there will be no problem and difference between deployment and production as a container can also refer to a specific version of a dependency so to be sure that in any case the used libraries don't get mixed up between different machines.
</p>
<p>
 To wrap an application into a container scaling becomes easier as through creating multiple instances of the same image there is the possibility to create a load balancer that redirects the requests and if one container gets too much traffic or dies unexpectedlly due to an error the whole system will not go down and it will be easy to check the performance and metrics of that specific container.
 <br>
 IMAGE3
</p>

<h2>Getting Started</h2>
<p>
 By running in the command line <code>docker container run hello-world</code> docker will look for the <code>hello-world</code> image locally and if not found will download it and before running the image a message informing about the download of the image will be shown. The actual output of this line is:
 <pre><code>Hello from Docker!
This message shows that your installation appears to be working correctly.

To generate this message, Docker took the following steps:
 1. The Docker client contacted the Docker daemon.
 2. The Docker daemon pulled the "hello-world" image from the Docker Hub.
    (amd64)
 3. The Docker daemon created a new container from that image which runs the
    executable that produces the output you are currently reading.
 4. The Docker daemon streamed that output to the Docker client, which sent it
    to your terminal.

To try something more ambitious, you can run an Ubuntu container with:
 $ docker run -it ubuntu bash

Share images, automate workflows, and more with a free Docker ID:
 https://hub.docker.com/

For more examples and ideas, visit:
 https://docs.docker.com/get-started/
 </code></pre>
 Images are to be consideres as immutable as they can't change after their creation.
 <br>
 To list all the locally available images run in the command line <code>docker image ls</code>
 <pre><code>REPOSITORY    TAG      IMAGE ID       CREATED         SIZE
hello-world   latest   feb5d9fea6a5   18 months ago   13.3kB
 </code></pre>
 Everytime an image is run it creates another container (so an instance). the file <code>Dockerfile</code> contains the commands used to build the image and its content resembles this:
 <pre><code># start image from a previously built image
FROM *image*:*tag*

# run cli command to install some dependencies
# or to perform an operation
RUN *cli command*

# run the cli command that will be execute on docker run
CMD *cli command executed on run*
 </code></pre>
 A dockerfile is written by a developer and docker <i>builds</i> the relative image and once it finishes it can also create containers (instances) of the build.
</p>
<p>
 A container contains only what is required to run the application. It is an isolated environment that can communicate with other containers through the TCP/UDP method.
 <br>
 To see all the container that are currently running run in the cli <code>docker container ls</code>
 <pre><code>CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES</code></pre>
 and to see all the exited containers add the <code>-a</code> flag: <code>docker container ls -a</code>
 <pre><code>
CONTAINER ID   IMAGE         COMMAND   CREATED        STATUS                     PORTS     NAMES
914f05a2f6c3   hello-world   "/hello"  7 seconds ago   Exited (0) 4 seconds ago              clever_jones
 </code></pre>
</p>
<p>
 The Docker cli is divided, under the hood, in three parts:
 <ol>
  <li>The docker cli tool that accepts a request</li>
  <li>The Docker daemon that manages the images and the containers</li>
  <li>A REST API used to communicate with the daemon</li>
 </ol>
 The Docker cli offers more than 50 different commands but the foundamentals and simplest ones are going to be used for 90% of the time:
 <dl>
  <dt>
   <code>docker image ls</code>
  </dt>
  <dd>Lists all the images</dd>
  <dt>
   <code>docker image pull *image*</code>
  </dt>
  <dt>
   gets an image from the image registry and downloads it online if there isn't one available  locally
  </dt>
  <dt>
   <code>docker container run *image name*</code>
  </dt>
  <dd>
   creates and runs a new container based on an image. <br> the <code>-d</code> flag runs it in detached state
  </dd>
  <dt>
   <code>docker container stop *container name or id*</code>
  </dt>
  <dd>
   stops a running container
  </dd>
  <dt>
   <code>docker image rm *image name*</code>
  </dt>
  <dd>
   Deletes a specific image
  </dd>
  <dt>
   <code>docker container ls</code>
  </dt>
  <dd>
   Lists all the container currently running
  </dd>
  <dt>
   <code>docker container ls -a</code>
  </dt>
  <dd>
   Lists all containers, both running and exited
  </dd>
  <dt>
   <code>docker container stop *container name or id*</code>
  </dt>
  <dd>
   Stops a running container
  </dd>
  <dt>
   <code>docker container rm *image id*</code>
  </dt>
  <dd>
   deletes a container by its id. It can also take the first few chars of the id and work anyway giving a warning only if there are more containers with an id that <i>start</i> the same. <br>Can accept multiple arguments
  </dd>
 </dl>
</p>

<p>
   Docker allows much more than that. containerization works with everything and by pulling from the internet the image of an operative system it will be possible to run the <code>tty</code> interface with the <code>-t</code> flag. The sole problem is that although this will output the interface it will still not send any of the inputs commands. To achieve it the flag <code>-i</code> will have to be put along so to actually send commands and receive output successfully. 
   <br>
   The flag <code>--name</code> allows for a custom name for a container. With a custom name will also be easier to stop or continue a container. Typing <code>docker pause *container name*</code> will stop the container with the specifed name and <code>docker unpause *container name*</code> will make it run again starting from where it was interrupted. 
   <br>
   Considering that containers are also piece of software that run in a fixed environment you can also communicate with them while they run. For this there is the flag <code>--exec</code>. An example use could be to have a running instance of a database and use the flag to query some specific data and to run additional commands to a running instance of an operative instance. What if we have deployed our software in a container with a specific linux distribution and we want to check something while is running? A very simple <code>docker exec -it *container name* bash</code> will open the bash command line in the terminal and with the <code>-it</code> flag it will be possible to interact with it.
   <br>
   Now that a lot has been put on the plate is time to see things in a more practical way.
</p>

<h2>Using containers</h2>
<p>
   Let's run a in a container a lightweight linux distribution: <a href="https://alpinelinux.org/">alpine</a>. When an image is not present locally docker will consult the official docker image registry at <a href="https://hub.docker.com/">docker hub</a>. There are, of course, other image registries like <a href="https://quay.io/">quay</a>.
   IMAGE 4
   Running <code>docker run alpine</code> will produce no output and although the container will be create it will immediately stop running
   IMAGE5
   Adding the <code>-it</code> the container will keep running and you'll be able to interact with it till you don't shut it down. By itself the containerized OS will open and will be possible to use it.
   IMAGE6
   to exit the container in this case the keyword <code>exit</code> will terminate the terminal and along with it the container too.
   IMAGE 7
</p>

<h2>Building images</h2>
<p>
   Every docker image is divided into <i>layers</i> that gets downloaded simultaneously if an image is collected from the web.
   IMAGE 8
   An image is built using a <b>Dockerfile</b>, a file which containers all the instructions regarding the various layers and behavior of the containers that will be instantiated. Docker can copy files, run commands on them and perform a wide array of actions into the container.
   <br>
   With a bash script called <code>hello.sh</code> with the following code
<pre><code>echo "Hi from a Docker container!"
</code></pre>
   and in the same directoy a file called simply <code>Dockerfile</code> with the following code it will be possible to containerize the file as it would be a fully blown application.
<pre><code># starting from the alpine image with tag 3.13 (which contains alpine 3.13)
FROM alpine:3.13

# set the '/app' directory as the working one
WORKDIR /app

# copy 'hello.sh' into the working directory (so '/app')
COPY hello.sh .

# 'RUN' is used to run terminal commands into the image. This commands gives permissions to the file
RUN chmod +x hello.sh

# 'CMD' is similar to RUN but runs when the image is created and there can be only one
CMD ./hello.sh
</code></pre>
   Running in the directoy with the two files the command <code>docker build . -t hello</code> will create an image following the various steps:
<pre><code>Sending build context to Docker daemon  3.072kB
Step 1/5 : FROM alpine:3.13
3.13: Pulling from library/alpine
72cfd02ff4d0: Pull complete 
Digest: sha256:469b6e04ee185740477efa44ed5bdd64a07bbdd6c7e5f5d169e540889597b911
Status: Downloaded newer image for alpine:3.13
   ---> 6b5c5e00213a
Step 2/5 : WORKDIR /app
   ---> Running in 238063f29511
Removing intermediate container 238063f29511
   ---> 993baed50ddb
Step 3/5 : COPY hello.sh .
   ---> 3818dc4285f3
Step 4/5 : RUN chmod +x hello.sh
   ---> Running in 152db25c29eb
Removing intermediate container 152db25c29eb
   ---> 7c1db39374e3
Step 5/5 : CMD ./hello.sh
   ---> Running in 587249037c6f
Removing intermediate container 587249037c6f
   ---> 1f3cbfdf42c0
Successfully built 1f3cbfdf42c0
Successfully tagged hello:latest
</code></pre>
   Using <code>docker run hello</code> will instantiate the container that will print <code>Hi from a Docker container!</code> for then exit automatically
</p>

<p>
   In the dockerfile you used certain keywords to build the images. You used <code>RUN</code> to execute some shell commands into the container and <code>CMD</code> but there is also a third one which is essential to develop serious images and if often confused with the other two: <code>ENTRYPOINT</code>.
   <br>
   <code>RUN</code> is the simplest of them all and for a simple enough image might also be enough. It simply runs some command line code into the container and that's. <code>CMD</code>, instead, is the final command of an image. There can be only one and at the end of the <code>Dockerfile</code> and it <b>gets executed during <code>docker run</code></b>.
   <br>
   <code>ENTRYPOINT</code> is similar to <code>CMD</code> with the foundamental difference that it allows for arguments when running the container. <code>CMD</code> can, and should, be used with <code>ENTRYPOINT</code> to set a default argument for the container creation. Let's say that a container needs a to container a parameter, something like a name. If the name is provided as an argument during docker run then the container would output something like
<pre><code>hi {name}!
</code></pre>
   otherwise it would automatically set <code>User</code> as default name. The code for this example would be
<pre><code>FROM alpine:3.13

WORKDIR /app

COPY hello.sh .

RUN chmod +x ./hello.sh

# write note about diff between exec and shell form
ENTRYPOINT ["sh", "hello.sh"]

CMD ["User"]</code></pre>
   In this code the file <code>hello.sh</code> is simply
<pre><code>echo "Hi" $1
</code></pre>
   Running the command <code>docker build . -t new-hello</code> will output the expected behavior and use <i>User</i> as default argument:
<pre><code>$ docker run new-hello
Hi User
$ docker run new-hello Mark
Hi Mark</code></pre>
   A similar behavior is also used when running images of operative systems. Running <code>docker run ubuntu</code>, for example, the default argument in the <code>CMD</code> is be <code>bash</code> so that running the image it will by default open the terminal to interact with it.
</p>

<h2>Storing a container's data</h2>
<!-- 
<h2>Volumes and bind mounts</h2> 
note: write difference between volumes and bind mounts
-->
<p>
   When data is generated by a container it exists only inside of itself. To store the generated data there are two interconnected ways: <i>volumes</i> and <b>bind mounts</b>. Bind mounts connect a file or directory of the container to the host machine and everything that happens into the file or directory of the container happens in the host machine file or folder too.
   <br>
   It uses the <code>-v</code> flag and accepts as argument a string that contains that path of both the local machine and container file or directory separated by a column. It is clearer with an example. Let's analyse the following image:
<pre><code>FROM alpine:3.13
WORKDIR /app

# creation of the file inside the container
RUN touch file.txt

# those two commands go together so that the 
# container can append the text to the bound
# file in the volumn and is possible to see
# the output at the same time
CMD echo "text" >> file.txt && cat file.txt

# to have this by itself in the CMD command
# would not work
#CMD cat file.txt
</code></pre>
   The container related to this image would by itself just print <code>text</code> but by binding <code>file.txt</code> with a file with the same name on the hosting machine the container's file will have persistence over time. The bind amount can happend only through file or directories with the same name so by creating a <code>file.txt</code> in the same directory of the Dockerfile the completed flag would be
<pre><code>-v "$(pwd)/file.txt:/app/file.txt"</code></pre>
   The value can be divided into three parts:
   <ol>
      <li><code>$(pwd)/file.txt</code> is the absolute path for the bound file on the host maching. <code>$(pwd)</code> indicates the current directory</li>
      <li><code>:</code> separates the absolute path of the file on the host machine from the one in the container</li>
      <li><code>/app/file.txt</code> is the absolute path of the file in the container</li>
   </ol>
   In the following image is possible to the result of running the container without using a volume and it simply outputs <code>text</code>. While using the <code>-v</code> flag the container's file and the one on the host machine are connected and everytime a new container gets run the content persists although the containers are different
   <br>
   IMAGE 9
   <br>
   The same holds true for directories too, of course. If in the current directory you delete <code>file.txt</code> and bind mount the <code>/app</code> directory with the current every change that will happen in the current directory will be persisted. Although to mount to files they need to have the same name this is not true for directories.
   <br>
   IMAGE 10
   <br>
   The most important note to bear in mind is that <i>routes are always absolute</i> and every attempt to use a relative one will result in an error.
<pre><code>docker run -v "$./file.txt:/app/file.txt" bind-mount-example
docker: Error response from daemon: create $./file.txt: "$./file.txt" includes invalid characters for a local volume name, only "[a-zA-Z0-9][a-zA-Z0-9_.-]" are allowed. If you intended to pass a host directory, use absolute path</code></pre>
</p>

<h2>Ports</h2>
<p>
   A running container can communicate using URLs and the most important is the address <code>127.0.0.1</code> usally called <code>localhost</code> and refers to the machine itself. This is important to know because by itself a container is simply a closed virtual space where code runs so if an application normally can be seen on localhost on a specific port if it runs in a container it will not communicate with the host machine by itself. Docker works with ports in two ways:
   <dl>
      <dt>Exposing a port</dt>
      <dd>To expose a certain port means that the container is listening on that port. It is useful especially for manual configuration and happens in the <code>Dockerfile</code></dd>
      <dt>Publishing a port</dt>
      <dd>To publish a port means to map a port of the container to one of the host machine so that it can be accessed while still running inside the container</dd>
   </dl>
   Let's analyse the following code for a <code>Dockerfile</code>:
<pre><code>FROM python

# will work also without exposing
# the port but is always better
# to not omit it
EXPOSE 8000

WORKDIR /app

RUN touch data.json
RUN echo '{"message":"Hello"}' >> data.json

CMD ["python", "-m", "http.server"]  
</code></pre>
   This code uses python to create a web server. It simply serves every file inside the current folder, in this case <code>/app</code>. By creating a json file with some data in it and it will be possible to receive the data when visiting the related url.
   <br>
   IMAGE 11
   <br>
</p>

<p>
   Docker is a foundamental part of general DevOps and although it might not be the tool that you'll be going to use the concepts of containers and virtualization are always the same and do not change between services. Containerization allows for an easier continous integration and continous deployment cycle but it still needs to be backed up by other practices to allow that everything runs as smooth as possible. Containers are a first but vital part of such processes.
</p>